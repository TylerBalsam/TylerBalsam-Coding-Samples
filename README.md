# TylerBalsam-Coding-Samples

A demonstrating of my coding ability, in both TFLearn/Tensorflow (Python) and C++. These are mostly coding snippets -- only the Floating Point Division program will run on its own.

This repository currently contains 3 projects:

1. An autoencoder dual-bottleneck colorization network with a classification output.

2. A novel autoencoder dual-bottleneck colorization network with a binary classification output.

3. A C++ project for a previous programming class, implementing a version of the lower level floating point division operations.

The two neural network projects are the result of my past year of work in this area. Here's a overview of each project:

## 1. Classification Colorization:

   This network is a dual bottlenecked classification network to colorize photos. The first part of the network is a typical autoencoder structure, and all of the outputs of each of the encoder blocks are bridged to the equivalent decoder block to help offload the flow of lower level details from the bottleneck. The secondary bottleneck is to reduce computation load and smooth the predicted classification distribution, since the output is a large classification output. In the secondary bottleneck, the output layer depth is squeezed to the original layer depth (default is 8), then expanded to a quarter, then half, then full depth classification depth. The minimal bin size I found in practice to produce realistically colored images was 25 per dimension, which results in an output depth of 50 total for both Cb and Cr, from the YCbCr colorspace.

   The YCbCr colorspace is used for its simplicity. Loss is a combination of neg-ln and a total variance loss. One interesting fact to note is that due to the classification output, the total variance loss is applied to the confidence levels of the network, and in practice this effectively smoothed output colors well (See Cool Trick #1 for more info). It additionally functioned as a regularization method. Output confidences were split to their respective Cb and Cr counterparts, and softmaxed across the depth of each split at each pixel.

### A few of the cool tricks I used:
   
   1. Confidence smoothing: This one comes first as it is a personal favorite of mine. I came up with this as a solution to smoothing issues for the classification network. Early on in the project I used a network with a Euclidean loss, and experienced quite a few problems with it. The primary one was the averaging issue that the MSE tends to cause. So I began building a classification output network to overcome the averaging issue. However, this had a few challenges, because while you could effectively smooth the real valued output of a network that had a Euclidean loss, directly smoothing the real valued output of a classification network was not possible. This is because the function that sampled the real values of the classification bins, ArgMax, is not differentiable, making it suited to a forward pass but not for training. In training, as with most classification networks, the neg-ln loss on the confidence for the correct output bin was used. So the main values that were acessible were the confidences, and these unfortunately came without any identifier of what bin they came from after the one-hot step eliminating the "useless" confidences.
   
   After some time spent working on this, I realized that many objects with a particular color or set of colors will often maintain that hue consistently throughout, often only varying in brightness. Because brightness was exclusively contained in the Y channel (the black and white channel, or luminosity channel), then the Cb and Cr channels for most objects of a single color would be uniform across, and the actual color shade would change to a more natural color gradient when the orginal black and white information was readded. See https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Barns_grand_tetons_YCbCr_separation.jpg/800px-Barns_grand_tetons_YCbCr_separation.jpg for an example of this. Because of the relative consistency of output values in the Cb and Cr space, I reasoned that the shape probability distributions of each color at that point should be similar as well, and so therefore most slices, if not all, should be similar in shape barring some edge cases. With those things in mind, I reasoned that I could effectively force generalization by applying total variance smoothing across the image. As the confidence values come in to the loss function, nearby slices are likely to be from the same distribution, and should be similar, and thus the distribution is smoothed the same way as it is built: sample by sample. Furthermore, the Cb and Cr channels are a strong candidate for this type of smoothing because the sheer number of relatively uniform values in the Cb and Cr channels outweighs the number of edges in those values, as the hard edges often lie in the Y channel, which we already have the data for. In practice, it worked very well. I found that small weighing values for this loss, like 1e-3, were more than sufficient for this particular network. One modification I found that made using this method easier was that instead of multiplying a one hot vector across the output distribution and taking the reduced mean neg-ln loss, applying a mask using the bin numbers pulled from the original image resulted in a neatly shaped, malleable network.

   2. Chroma Subsampling: Chroma subsampling was one trick that has been very effectively used for web image compression, video compression and many other things. It is an essentially "free" optimization. The core concept is that our eyes are less sensitive to color resolution than luminosity resolution in images (see https://upload.wikimedia.org/wikipedia/commons/0/06/Colorcomp.jpg) for more details. In earlier tests of a less memory-intensive form of this network (using a pretrained VGG-16 network as an encoder network), I found that I was able to reduce the color resolution without any blatant issues. The current variation of the classification network in this repository does not use a reduced color resolution for the sake of modularity and personal experimentation, but those past experiments shown strong potential for greatly reducing the output size of the network as a late optimization. The 48 hour VGG validation set photos in the /results folder demonstrates the VGG base variant of this network, which did use that upsampling. That particular folder is very early on in the network's training, so it is a demonstrator of potential only, and not full capacity.

   3. Subpixel Convolutions: Instead of using transposed convolutions, which are a sparse method of information generation, I opted to use subpixel covolutional layers after the bottleneck (See https://github.com/Tetrachrome/subpixel). Something I noted was that their code seemed to function in a similar fashion to the tf.batch_to_space() operation, and so I implemented that with a block size of 2. It appeared to work effectively in place of transposed convolutions, and appeared to almost completely remove the "checkerboard" subpatterns seen often in generative networks that use transposed convolutions. One future extension to this change that I have yet to experiment with is the number of of filters for the convolution directly before the subpixel convolution layer. Currently it is unchanged, but I see the potential for a good results-to-parameters improvement there. One reason is because the subpixel convolutions reduce the depth of the incoming layer stack by a factor of four, and thus the layer bridge from the encoder network that gets concatenated to the output stack of the incoming output from the decoder network is actually taller than the generated stack! In a potentional worst case scenario where incoming information from the lower layers of the encoder network is useless, the decoder can ignore those layers and shift its weights to focus only on the layers coming off of the decoder stack, but I'm not sold on the merits of repeatedly bottlenecking each decoder's depth.

### Problems for consideration
   
   1. Currently, the value drawn from the forward pass of the colorization network is the value with the maximum confidence. This is because without some sort of object segmentation, colors drawn for a particular object from the predicted binned distribution would be speckled all over the map, and possibly incorrect since a 3D topographical reconstruction of the distribution would be from 2 2D representations, which presents information loss scenarios. One possible solution for this includes training a segmentation network on top of a pretrained version of this network and pulling a draw from the similar distributions across each segmented area. Due to the lower need for color resolution in the YCbCr colorspace, this would be a fairly fault tolerant. However, adapting a segmentation network to inputs of 224x224x25x2 could be extremely difficult/memory intensive, and I believe there are better solutions out there.

   2. The network output, at 224x224x50 (which is converted to 224x224x25x2 for YCbCr binning), takes up a tremendous amount of memory in training and heavily limits batch size. Possible solutions include the aforementioned chroma subsampling and/or using lower layers of the VGG-16 network as a fixed encoder network and training only the middle section and decoder network. With the encoder network no longer encumbered with the memory requirement of the gradients for traiing, this allows for much larger batch sizes with moderately good performance. See results/VGG-16-Encoder-48-Hours for the uncurated validation set about 48 hours into training on a GTX 1070 with a batch size of 50 using this method. An alternate solution would be to convert to an architecture similar to Conditional Adversarial Nets (see https://phillipi.github.io/pix2pix/). One solution I have implemented with promising results has been the below network, a Binary Classification Network. I believe it deserves to stand on its own due to its novelty.

## 2. Binary Classification Colorization:

   After a few months working with the above classification network, I decided to look for a better way to express its output. As I've mentioned above, the main reason I had created the classification network was to separate the colors into distinct bins, thus solving the brown color issue that the Euclidean loss created on the original network. The Euclidean loss in an earlier network I used chose a brown color for many objects with a high color variance because it was averaging the colors it saw in practice -- a great mode of convergence for single-color objects like the grass and sky, but terrible for cars, dresses, balloons, and so on. One interesting property of the classification network was that it gave a partial projection of the probability curve of the color distribution at that point for each point. However, I quickly ran into problems with this methodology. One was that while you could plot the bin probabilities for the CB and CR dimensions on the x and y axis, and create a 3D "Probability Map" containing hills and valleys of probabilities for each bin at a 2D location, it was a lossy compression. Here's why.
   
   By transcribing the peaks and the high points of the distribution from two different perspectives in 2D (the Cb and Cr plots), the valleys and smaller hills hidden "behind" the peaks in the 3D contour plot are lost -- leaving only guesses as to their real height. My solution to this was to only sample the highest point on each axis, which we can know is always correctly projected. It did achieve the effect of producing more distinctly vivid colors, but this method also discarded the rest of that information. One way to get around this would be to flatten every slice of that contour plot to 1D, and instead of plotting a 25x25 space at each point, plot a 625 bin plot at each point. However, even at a half scale of 112x112 pixels with chroma subsampling, this bin size would be almost pointless from a memory-to-value standpoint with the resources I have. So I took the thought process in the other direction, reasoning that if I was only going to draw the highest value from the distribution each time, and I could not de-abstract to a lower, more explicit set of dimensions, then the only way I could go was up. In effect, I could effectively discard the rest of the data, those lower peaks, and write it off as entropy. Going from 1D to 2D had that effect of compressing the expressed data at the cost of entropy, so I drew plans for a draw of the data in 4 rows of length N, which each mapped to bin numbers on the joing Cb and Cr plane. I kept raising the dimensions and shortening the sequence until I eventually ended up with N rows of length 2, which was of course a binary representation. In practice, this resulted in an output of 2x6x2 bins, a pair of 1 and 0 bins for each of the 6 digits, and 1 set of digits for Cb bin number and 1 for the Cr bin number. (Note: one could abstract it to a 1xNx2 system and flatten the CbCr bins to one dimension, but for simplicity's sake in implementing and testing the concept I kept them separate.)

   A negative log likelihood loss is applied to the bins (softmaxed along each pair of 1 and 0 bins) during training time, and during the forward pass the bin with the highest confidence (i.e. > .5) is selected. In the forward pass, the binary digit at each point is converted to a color integer, which represents the Cb or Cr bin, respectively. Then that bin is converted to the lowest value in the bin, and stacked on the original BW image layer, converted and saved to a .png file.

   While it does not converge as quickly as the normal classification network, which did not converge as quickly as a network with a Euclidean Loss, it has exceeded any expectations I could have had for it. After some modifications to the original design, I was able to get it to converge well on the sanity test. I am currently working on optimizing it for convergence on a smaller test set, and initial results look promising. For the sanity test and the confidence map, see the relevant folder under colorization_binary. One thing to note is that the confidence map for this network on the sanity test, compared to the classification network, is more speckled. This could be either an indicator of a more fragile generalization, or just a side effect of the sanity test, since the sanity test tests the network convergence on just one image. From my past experience, lowering speckling in the future will be a combination of adjusting the total variance loss constant, the learning rate, and increasing the epoch numbers during training time. Running post-processing is also an option as well.

As it stands, it's a network I'm proud of. I personally belive that it has a lot of future potential if I can continue learning and adapting to its unique traits.

If you have any questions about this network, or the first network, feel free to drop me a line at my personal email, tbcharger@gmail.com.

## 3. Floating Point Division:

   This was for a class project in the fall of 2015, and is a more vanilla demonstration of my coding ability. It is a relatively simple, lower level program to do floating point division on an input file of two floating point integers in hex, each pair separated by a space, one per line. Compared to the networks above, it may be more boring, but I think it's a good demonstration of my (somewhat) lower level abilities since the above networks were mostly implemented using high-level APIs.
